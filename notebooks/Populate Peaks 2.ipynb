{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from met_explore.models import *\n",
    "from met_explore.serializers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import json\n",
    "import collections\n",
    "from difflib import SequenceMatcher\n",
    "from collections import OrderedDict\n",
    "PROTON = 1.007276\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print (cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "for f in files:\n",
    "   print (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON file in format {sample: {peak_id:intensity, peak_id:intensity...}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of peak objects to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('67_peak_cmpd_export_1.json') as json_peak_data:\n",
    "    peak_details = json.load(json_peak_data)\n",
    "    print (len(peak_details['pid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe returned from PiMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peak_details_df= pd.read_json('67_peak_cmpd_export_1.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on adduct type on accepting M+H and M-H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_adducts = (peak_details_df['adduct']=='M+H') | (peak_details_df['adduct']=='M-H')\n",
    "f_adducts = peak_details_df[selected_adducts]\n",
    "display(f_adducts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a small assert function for this. - to check we have the correct numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = f_adducts['frank_annot'].notnull()\n",
    "f_annots = f_adducts[test]\n",
    "display(f_annots.shape)\n",
    " #1234 rows with a FrAnk annotation\n",
    "test2 = f_adducts['identified']==\"True\"\n",
    "display(f_adducts[test2].shape)\n",
    "#380 rows with identified = True\n",
    "test3 =  (f_adducts['identified']==\"True\") & (f_adducts['frank_annot'].notnull())\n",
    "display(f_adducts[test3].shape[0])\n",
    "# 160 rows have identified and true\n",
    "# The number of ID and annotated peaks should be:\n",
    "1234+380-160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected on identified = true and/or frank annotations exist.\n",
    "#### (have selected_df as a member variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_annot = (f_adducts['frank_annot'].notnull()) | (f_adducts['identified']=='True')\n",
    "selected_df = f_adducts[with_annot]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = add_neutral_masses(selected_df)\n",
    "display(selected_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the df in two sections - first the Identified rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = list(selected_df.columns.values)\n",
    "final_id_df = pd.DataFrame(columns=headers)\n",
    "print(\"final id df\")\n",
    "display(final_id_df)\n",
    "all_id_df = selected_df[selected_df.identified=='True']\n",
    "\n",
    "id_df = get_id_cmpds(all_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sid_df = selected_df[selected_df.sec_id==2590]\n",
    "cmpd_id = sid_df[sid_df.db==\"stds_db\"]['cmpd_id'].values\n",
    "print (cmpd_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method (atm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = list(selected_df.columns.values)\n",
    "final_df = pd.DataFrame(columns=headers)\n",
    "unique_sec_ids = selected_df['sec_id'].unique()\n",
    "\n",
    "#unique_sec_ids = [2212,3976]\n",
    "\n",
    "for sid in unique_sec_ids:\n",
    "    #Collect a single sec_id into a DF\n",
    "    sid_df = selected_df[selected_df.sec_id==sid]\n",
    "    print(\"The single SID DF is\")\n",
    "    display(sid_df)\n",
    "    #If the peak has an identified compound then keep that\n",
    "    identified_df = sid_df[sid_df.identified=='True']\n",
    "    print (\"The identified df is: \")\n",
    "    display(identified_df)    \n",
    "    new_row = None\n",
    "    #If some of the rows have compounds that have identified=True\n",
    "    if not identified_df.empty:\n",
    "                  \n",
    "        #Check if there are more than one standard compounds for this sid\n",
    "        standard_cmpds = sid_df[sid_df.db=='stds_db']\n",
    "        num_std_cmpds = standard_cmpds.shape[0]\n",
    "        \n",
    "        # If there is only one standard compound add this to the final DF and collect identifiers. \n",
    "        if (num_std_cmpds == 1):\n",
    "            \n",
    "            print (\"we have only one standard compound\")\n",
    "            cmpd_id = sid_df[sid_df.db==\"stds_db\"]['cmpd_id'].values[0]\n",
    "            new_row = get_peak_by_cmpd_id(sid_df, cmpd_id)            \n",
    "        \n",
    "        #Here we have the senario that more that 1 standard compound has been identified and we \n",
    "        #want to select a standard compound if possible \n",
    "        if (num_std_cmpds > 1):\n",
    "            print (\"the number of standard compounds for sid is\", sid, \"is\", num_std_cmpds)\n",
    "            new_row = select_standard_cmpd(sid_df, standard_cmpds)\n",
    "            \n",
    "\n",
    "        #If a new_row has been returned for this SID - add it to the final_df\n",
    "        if new_row is not None:\n",
    "            \n",
    "            print(\"we are adding the row for sid\", sid)\n",
    "            display(pd.DataFrame(new_row).T)\n",
    "            final_df = final_df.append(new_row)  \n",
    "\n",
    "        #If the new_row has not been determined for this SID\n",
    "        else:\n",
    "            \n",
    "            unique_cmpd_ids = sid_df['cmpd_id'].unique()\n",
    "\n",
    "            #For each unique compound id add a row to the final df, this will produce duplicates for later    \n",
    "            for ucid in unique_cmpd_ids:\n",
    "                \n",
    "                new_row = get_peak_by_cmpd_id(sid_df, ucid)   \n",
    "                print(\"we are adding the row: for sid\", sid)\n",
    "                display(pd.DataFrame(new_row).T)\n",
    "                final_df = final_df.append(new_row)   \n",
    "                \n",
    "    # Else nothing identified so look at the fragmentation data.\n",
    "    else:\n",
    "        #Get all the rows for this secondary ID\n",
    "        print (\"nothing identified here so get best match FrAnk compound\")\n",
    "        new_row = select_on_frank(sid_df)\n",
    "        print(\"we are adding the row: for sid\", sid)\n",
    "        display(pd.DataFrame(new_row).T)\n",
    "        final_df = final_df.append(new_row)  \n",
    "\n",
    "        \n",
    "\n",
    "# final_df = check_for_duplicates(final_df, chosen_std_cmpds) \n",
    "\n",
    "print (\"This is the final df\")\n",
    "display (final_df)\n",
    "\n",
    "print (\"There are\", final_df['sec_id'].nunique(), \"unique compounds out of\", final_df.shape[0], \"rows added\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_on_frank(sid_df):\n",
    "    compound_names = sid_df['compound'].values\n",
    "    name_match_dic = {}\n",
    "\n",
    "    frank_annots = sid_df['frank_annot']\n",
    "    num_att = len(frank_annots.iloc[0])\n",
    "    #Look for the number of unique frank annotations -assuming each set of one annotation conatins 5 items.\n",
    "    # no_attributes = (len(frank_annots[0]))\n",
    "    # print (no_attributes)\n",
    "    unique_f_att = set()\n",
    "    for f in frank_annots:\n",
    "        items = f.items()\n",
    "        for i in items:\n",
    "            unique_f_att.add(i)\n",
    "    print (\"the unique frank attribues are\", unique_f_att)\n",
    "    #If we have only one frank_annot i.e. all the colums have the same FrAnK Annotation returned\n",
    "    if len(unique_f_att)==num_att:\n",
    "        \n",
    "        single_annot = frank_annots.iloc[0]\n",
    "        \n",
    "        for pimp_cmpd_name in compound_names:\n",
    "            #Find the best fit for to frank.\n",
    "            frank_cmpd_name = single_annot['frank_cmpd_name']\n",
    "            m = SequenceMatcher(None, frank_cmpd_name, pimp_cmpd_name)\n",
    "            name_match_dic[pimp_cmpd_name]=m.ratio()\n",
    "            \n",
    "            \n",
    "        if name_match_dic:\n",
    "            print (name_match_dic)\n",
    "            max_value = max(name_match_dic.values())  # maximum value\n",
    "            max_keys = [k for k, v in name_match_dic.items() if v == max_value]\n",
    "            max_key = max_keys[0]\n",
    "            \n",
    "            \n",
    "            if (max_value >= 0.5):\n",
    "                print (\"max_key to grab row\", max_key)\n",
    "         \n",
    "            \n",
    "                #If the match is less than 50% just take the frank annotation instead.\n",
    "\n",
    "                #Take the first compound with this name\n",
    "                cmpd_id = sid_df[sid_df['compound'] == max_key]['cmpd_id'].iloc[0]\n",
    "                new_row = sid_df[sid_df['compound'] == max_key].iloc[0]   \n",
    "\n",
    "                ucid = new_row['cmpd_id']\n",
    "                cmpd_rows_df = sid_df[sid_df.cmpd_id==ucid]\n",
    "                identifiers = get_all_identifiers(cmpd_rows_df)\n",
    "                new_row.at['identifier'] = identifiers\n",
    "            \n",
    "            \n",
    "            if (max_value < 0.5):\n",
    "                \n",
    "                print (\"max_value is < 0.5 (\", max_value, \") and so taking frank annot\")\n",
    "                \n",
    "                new_row = get_frank_annot(sid_df)\n",
    "            \n",
    "        print (\"The FrAnK probability score is\", single_annot['probability'] )\n",
    "\n",
    "   \n",
    "    else:         \n",
    "        print(\"Looks like there is more than one set of attributes for this SID, who knew this could happen??\")\n",
    "        sys.exit()\n",
    "              \n",
    "    return new_row\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frank_annot(sid_df):\n",
    "    \n",
    "    new_row = None\n",
    "    frank_annots = sid_df['frank_annot']\n",
    "    single_annot = frank_annots.iloc[0] \n",
    "    \n",
    "    if any(single_annot):\n",
    "        \n",
    "        print (\"single_annot type\", type(single_annot))\n",
    "  \n",
    "    \n",
    "        (\"the FrAnk annotation is: \", single_annot)\n",
    "        new_row = sid_df.iloc[0]\n",
    "        new_row.at['compound'] = single_annot['frank_cmpd_name']\n",
    "    \n",
    "    \n",
    "\n",
    "        #Get all the frAnk identifiers for a single PiMP compound (could be one).\n",
    "        identifiers=[]\n",
    "        identifier_keys =['inchikey','cas_code','hmdb_id']\n",
    "        for i in identifier_keys:\n",
    "                identifiers.append(single_annot[i])\n",
    "        new_row.at['identifier']=identifiers\n",
    "    \n",
    "    return new_row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This should be used if more than 1 standard compound has been identified for a single peak.\n",
    "\"\"\"\n",
    "This takes in a list of the standard compounds for a unique peak ID and returns a \n",
    "new row if a preferred standard comound can be identified. If not, None is returned.\n",
    "\"\"\"\n",
    "def select_standard_cmpd(sid_df, standard_cmpds):\n",
    "    #For each of the standard compounds identified for the peak \n",
    "    new_row = None\n",
    "    display(standard_cmpds)\n",
    "    name_match_dic = {}\n",
    "\n",
    "\n",
    "    for i in standard_cmpds['identifier'].unique():\n",
    "        pimp_cmpd_name = standard_cmpds[standard_cmpds.identifier==i]['compound'].iloc[0]\n",
    "        annotation = standard_cmpds['frank_annot'].values[0] #Get the value in the cell\n",
    "        \n",
    "        #If there is a FrAnK annotation get the best name match       \n",
    "        if (annotation is not None):\n",
    "            \n",
    "            frank_cmpd_name = annotation['frank_cmpd_name']\n",
    "            m = SequenceMatcher(None, frank_cmpd_name, pimp_cmpd_name)\n",
    "            name_match_dic[pimp_cmpd_name]=m.ratio()\n",
    "    \n",
    "            print (name_match_dic)\n",
    "            max_value = max(name_match_dic.values())  # maximum value\n",
    "            max_keys = [k for k, v in name_match_dic.items() if v == max_value]\n",
    "            max_key = max_keys[0]\n",
    "            print (\"max_key to grab row\", max_key)\n",
    "\n",
    "\n",
    "            cmpd_id = standard_cmpds[standard_cmpds['compound'] == max_key]['cmpd_id'].iloc[0]\n",
    "            new_row = standard_cmpds[standard_cmpds['compound'] == max_key].iloc[0]   \n",
    "\n",
    "            ucid = new_row['cmpd_id']\n",
    "            cmpd_rows_df = sid_df[sid_df.cmpd_id==ucid]\n",
    "            identifiers = get_all_identifiers(cmpd_rows_df)\n",
    "            new_row.at['identifier'] = identifiers       \n",
    "\n",
    "    print (\"sending back a new row from select_standard_compound of type \", type(new_row))\n",
    "    return new_row\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_by_cmpd_id(sid_df, ucid): \n",
    "    \n",
    "    new_row = None #Clear the new row at this stage\n",
    "    cmpd_rows_df = sid_df[sid_df.cmpd_id==ucid]\n",
    "    identifiers = get_all_identifiers(cmpd_rows_df)\n",
    "    print (\"The returned identifiers are \",identifiers)\n",
    "    cmpd_id = cmpd_rows_df['cmpd_id'] == ucid\n",
    "    \n",
    "    #Take the row with std_db just to remember this was identified.\n",
    "    db = cmpd_rows_df['db'] == 'stds_db'\n",
    "    new_row = cmpd_rows_df[cmpd_id & db].iloc[0]\n",
    "    new_row.at['identifier'] = identifiers\n",
    "    \n",
    "    print ('Returning new_row by cmpd_id')\n",
    "    \n",
    "    return new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates in the final identified DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate peaks are ones which had more than one standard compound identified for them.\n",
    "\n",
    "duplicate_df = final_id_df[final_id_df.sec_id.duplicated()]\n",
    "display(duplicate_df)\n",
    "\n",
    "MASS_TOL = 0.1\n",
    "RT_TOL = 5\n",
    "\n",
    "dup_ids = duplicate_df['sec_id'].values\n",
    "\n",
    "for dupid in dup_ids:\n",
    "    \n",
    "    dup_peaks = final_id_df[final_id_df.sec_id==dupid]\n",
    "    \n",
    "    print (\"duplicate peaks are: \")\n",
    "    display (dup_peaks)\n",
    "        \n",
    "#     Assuming that all of the data for these duplicate peaks are the same, take the firt value.\n",
    "\n",
    "    neutral_mass = dup_peaks['neutral_mass'].iloc[0]\n",
    "    rt = dup_peaks['rt'].iloc[0]\n",
    "    adduct = dup_peaks['adduct'].iloc[0]\n",
    "\n",
    "    \n",
    "    min_mass = neutral_mass-MASS_TOL\n",
    "    max_mass = neutral_mass+MASS_TOL\n",
    "   \n",
    "    min_rt = rt-RT_TOL\n",
    "    max_rt = rt+RT_TOL\n",
    "    \n",
    "    \n",
    "    mass_match = final_id_df['neutral_mass'].between(min_mass, max_mass, inclusive=True)\n",
    "    rt_match = final_id_df['rt'].between(min_rt, max_rt, inclusive=True)\n",
    "    no_duplicates = final_id_df['sec_id'] != dupid\n",
    "    \n",
    "   # If we have stored another peak that within a neutral_mass and rt tolerance.\n",
    "   \n",
    "    matching_cmpd_df = final_id_df[mass_match & rt_match & no_duplicates]\n",
    "    \n",
    "    print (\"Other peaks that match the duplicate peaks are:\")                                  \n",
    "    display(matching_cmpd_df)\n",
    "    \n",
    "    \n",
    "    if matching_cmpd_df.index.any():\n",
    "    \n",
    "        print (\"There are matching compounds!!!\")\n",
    "        #Calculate some values that may be useful.\n",
    "\n",
    "#         matching_cmpd_df.loc[:,'abs_mdif'] = (matching_cmpd_df['neutral_mass'] - neutral_mass).abs()\n",
    "#         matching_cmpd_df.loc[:,'abs_rtdif'] = (matching_cmpd_df['rt'] - rt).abs()\n",
    "\n",
    "    #   Peak ids for the peaks that match the duplicate peak. \n",
    "        match_sec_ids = matching_cmpd_df['sec_id'].values\n",
    "\n",
    "        for m in match_sec_ids:\n",
    "\n",
    "#             dup_names = final_id_df[final_id_df.sec_id==dupid]['compound'].values\n",
    "            \n",
    "            \n",
    "            dup_names = dup_peaks['compound'].values\n",
    "            match_name = matching_cmpd_df[matching_cmpd_df.sec_id==m]['compound'].iloc[0]\n",
    "            match_adduct = matching_cmpd_df[matching_cmpd_df.sec_id==m]['adduct'].iloc[0]\n",
    "            print (\"dup_names \", dup_names)\n",
    "            for dup_name in dup_names:\n",
    "                print (dup_name)\n",
    "                keeping = False\n",
    "                if dup_name == match_name:\n",
    "                    print (\"names are the same\")\n",
    "                    if match_adduct != adduct:\n",
    "                        print (\"adducts are different\")\n",
    "                        print (\"keeping this:\")\n",
    "                        display (dup_peaks[dup_peaks['compound']==dup_name])\n",
    "                        keeping = True\n",
    "#                     final_id_df.index[final_id_['BoolCol'] == True].tolist()\n",
    "                    else:\n",
    "                        print (\"ADDUCTS ARE THE SAME\")\n",
    "                        \n",
    "                if not keeping:\n",
    "                \n",
    "                    print (\"should delete this:\")\n",
    "                    display (dup_peaks[dup_peaks['compound']==dup_name])\n",
    "                    index_to_drop = dup_peaks.index[dup_peaks['compound'] == dup_name].tolist()\n",
    "\n",
    "                    print (\"trying to drop index \", index_to_drop[0])\n",
    "                    print (\"Trying to delete\")\n",
    "                    display(dup_peaks.loc[index_to_drop])\n",
    "\n",
    "                    final_id_df = final_id_df.drop(index_to_drop)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print (\"no matching compounds in the final DF\")\n",
    "        \n",
    "        \n",
    "        dup_names = dup_peaks['compound'].values\n",
    "        for n in dup_names:\n",
    "            m_name = final_id_df['compound']==n\n",
    "            match_name = final_id_df[m_name & no_duplicates]\n",
    "            print (\"This peak matches on name:\")\n",
    "            match_name.loc[:,'abs_mdif'] = (match_name['neutral_mass'] - neutral_mass).abs()\n",
    "            match_name.loc[:,'abs_rtdif'] = (match_name['rt'] - rt).abs()\n",
    "\n",
    "            display(match_name)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    #Check if the mass and RT match a peak of the opposide adduct that we have already stored in the final_id_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     [3722 2226 1147 2589 2909 2742]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate peaks\n",
    "\n",
    "### For each heading the JSON files give a dictionary where the key is the unique ID/row and the value is that of the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hearders returned from JSON\n",
    "peak_attributes = ['sec_id','mass','rt','polarity','compound','formula','identified','identifier','frank_annot','adduct']\n",
    "\n",
    "# These are the field names in the Peak model\n",
    "peak_model_attributes = ['psec_id','m_z','rt','polarity','cmpd_name','cmpd_formula','identified','cmpd_identifiers','frank_anno','adduct']\n",
    "\n",
    "#Dictionary to relate the JSON key names to the peak model column names.\n",
    "json_model_dict = collections.OrderedDict()\n",
    "\n",
    "for (pa, pma) in zip(peak_attributes, peak_model_attributes):\n",
    "    json_model_dict[pa] = pma\n",
    "\n",
    "print (json_model_dict)\n",
    "\n",
    "\n",
    "#Dictionary that stores the Peak model attributes and values.\n",
    "data_dict ={}\n",
    "\n",
    "\n",
    "#For each peak in the JSON file make a new Peak object\n",
    "for i in range (0,len(peak_details['pid'])):\n",
    "    for attribute in json_model_dict:\n",
    "       # Get the dictionary for this attribute\n",
    "        current_attribute = peak_details[attribute]\n",
    "        if json_model_dict[attribute]=='frank_anno':\n",
    "            data_dict[json_model_dict[attribute]] = str(current_attribute[str(i)]) #Frank info from Dict to string for the moment\n",
    "        else:\n",
    "            data_dict[json_model_dict[attribute]] = current_attribute[str(i)]\n",
    "    print(data_dict)\n",
    "        \n",
    "    #Add the peak to the DB\n",
    "    peak_serializer = PeakSerializer(data=data_dict)\n",
    "    if peak_serializer.is_valid():\n",
    "        db_peak = peak_serializer.save()\n",
    "        print (\"peak saved \", db_peak.pid)\n",
    "    else:\n",
    "        print (peak_serializer.errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a compound ID return all of the identifiers for this compound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a FrAnK annotation dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add neutral masses to the DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_neutral_masses(selected_df):\n",
    "    masses = selected_df['mass'].values\n",
    "    adducts = selected_df['adduct'].values\n",
    "    print (\"adding neutral masses to the dataframe\")\n",
    "    # neutral_masses = [get_neutral_mass(mass, adduct) for mass in masses for adduct in adducts]\n",
    "    neutral_masses = []\n",
    "    joint_list =[masses, adducts]\n",
    "    # mass_adducts = OrderedDict(zip(selected_df.mass, selected_df.adduct))\n",
    "\n",
    "    mass_adducts = list(zip(*joint_list))\n",
    "    for ma in mass_adducts:\n",
    "        mass = ma[0]\n",
    "        adduct = ma[1]\n",
    "        neutral_mass = get_neutral_mass(mass, adduct)\n",
    "        neutral_masses.append(neutral_mass) \n",
    "    print(len(neutral_masses))\n",
    "    selected_df.loc[:,'neutral_mass'] = np.asarray(neutral_masses)\n",
    "\n",
    "    return selected_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Small function to return a netral mass given the m/z and the adduct.\n",
    "def get_neutral_mass(mass, adduct):\n",
    "    if adduct == 'M+H':\n",
    "        neutral_mass = mass-PROTON\n",
    "    elif adduct == 'M-H':\n",
    "        neutral_mass = mass+PROTON\n",
    "    else:\n",
    "        print (\"What's going on?\")\n",
    "    \n",
    "    return neutral_mass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a peak dataframe get all the unique identifiers in the DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a df and returns a list of identifiers\n",
    "def get_all_identifiers(cmpd_rows_df):\n",
    "\n",
    "    #Take all the rows relating to a unique compound\n",
    "    num_rows = cmpd_rows_df.shape[0]\n",
    "    print (\"the number of rows are \", num_rows)\n",
    "\n",
    "    #Get all the identifiers for a single PiMP compound (could be one but want this as list).\n",
    "    identifiers=[]\n",
    "    for i in range(0,(num_rows)):\n",
    "        new_id = cmpd_rows_df.iloc[i]['identifier']\n",
    "        identifiers.append(new_id)\n",
    "\n",
    "        #Take one of the rows for this compound, add identifiers and save in the final df.\n",
    "    \n",
    "    return identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
